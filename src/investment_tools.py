"""
å®ç”¨æŠ•èµ„å·¥å…·æ¨¡å—
æä¾›æŠ•èµ„é‡‘é¢è®¡ç®—ã€è°ƒä»“ä¿¡å·ã€ä¸šç»©å½’å› ç­‰å®ç”¨åŠŸèƒ½
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)


class InvestmentCalculator:
    """æŠ•èµ„è®¡ç®—å™¨"""

    def __init__(self, initial_capital: float = 1000000):
        """
        åˆå§‹åŒ–æŠ•èµ„è®¡ç®—å™¨

        Args:
            initial_capital: åˆå§‹èµ„é‡‘ï¼Œé»˜è®¤100ä¸‡
        """
        self.initial_capital = initial_capital

    def calculate_position_sizes(self, weights: np.ndarray,
                                portfolio_value: float,
                                etf_prices: np.ndarray) -> Dict[str, Dict[str, float]]:
        """
        è®¡ç®—å…·ä½“æŒä»“æ•°é‡

        Args:
            weights: æŠ•èµ„ç»„åˆæƒé‡
            portfolio_value: æŠ•èµ„ç»„åˆæ€»ä»·å€¼
            etf_prices: ETFä»·æ ¼åˆ—è¡¨

        Returns:
            æŒä»“ä¿¡æ¯å­—å…¸
        """
        positions = {}

        for i, (weight, price) in enumerate(zip(weights, etf_prices)):
            target_value = weight * portfolio_value
            shares = int(target_value / price / 100) * 100  # æŒ‰æ‰‹æ•°ï¼ˆ100è‚¡ï¼‰è®¡ç®—

            actual_value = shares * price
            actual_weight = actual_value / portfolio_value

            positions[f'ETF_{i}'] = {
                'target_weight': weight,
                'actual_weight': actual_weight,
                'target_value': target_value,
                'actual_value': actual_value,
                'shares': shares,
                'price': price,
                'weight_diff': actual_weight - weight
            }

        # è®¡ç®—æ±‡æ€»ä¿¡æ¯
        total_actual_value = sum(pos['actual_value'] for pos in positions.values())
        cash_balance = portfolio_value - total_actual_value

        positions['summary'] = {
            'total_target_value': portfolio_value,
            'total_actual_value': total_actual_value,
            'cash_balance': cash_balance,
            'cash_percentage': cash_balance / portfolio_value,
            'total_shares': sum(pos['shares'] for pos in positions.values())
        }

        return positions

    def project_portfolio_growth(self, annual_return: float,
                               annual_volatility: float,
                               years: int = 5,
                               simulations: int = 5000) -> Dict[str, Any]:
        """
        ç°å®ç‰ˆæŠ•èµ„ç»„åˆå¢é•¿é¢„æµ‹

        Args:
            annual_return: å¹´åŒ–æ”¶ç›Šç‡
            annual_volatility: å¹´åŒ–æ³¢åŠ¨ç‡
            years: é¢„æµ‹å¹´æ•°
            simulations: è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ¬¡æ•°

        Returns:
            å¢é•¿é¢„æµ‹ç»“æœ
        """
        logger.info(f"ğŸ§® å¼€å§‹å¢é•¿é¢„æµ‹: {annual_return:.1%}å¹´åŒ–æ”¶ç›Š, {annual_volatility:.1%}æ³¢åŠ¨ç‡, {years}å¹´")

        # ä½¿ç”¨å¹´é¢‘ï¼Œå¹¶è°ƒæ•´æ³¢åŠ¨ç‡ä»¥åæ˜ æ›´ç°å®çš„é£é™©
        final_values = []
        yearly_paths = []
        max_drawdowns = []

        # å¯¹äºé«˜æ”¶ç›Šç‡ï¼Œå¢åŠ æ³¢åŠ¨ç‡ä»¥åæ˜ çœŸå®é£é™©
        adjusted_volatility = max(annual_volatility, 0.3)  # è‡³å°‘30%å¹´åŒ–æ³¢åŠ¨ç‡

        # å¦‚æœæ”¶ç›Šç‡å¼‚å¸¸é«˜ï¼Œè¿›ä¸€æ­¥è°ƒæ•´
        if annual_return > 0.3:  # è¶…è¿‡30%å¹´åŒ–æ”¶ç›Š
            adjusted_volatility = max(adjusted_volatility, annual_return * 0.8)  # æ³¢åŠ¨ç‡è‡³å°‘æ˜¯æ”¶ç›Šçš„80%

        for sim_idx in range(simulations):
            if sim_idx % 1000 == 0 and sim_idx > 0:
                logger.info(f"ğŸ“Š æ¨¡æ‹Ÿè¿›åº¦: {sim_idx}/{simulations}")

            # ç”Ÿæˆå¹´æ”¶ç›Šç‡ï¼Œæ·»åŠ ç°å®çš„å¸‚åœºå†²å‡»
            yearly_returns = np.random.normal(annual_return, adjusted_volatility, years)

            # æ·»åŠ å¸‚åœºå†²å‡»å› ç´ ï¼ˆéšæœºé»‘å¤©é¹…äº‹ä»¶ï¼‰
            for i in range(years):
                if np.random.random() < 0.1:  # 10%æ¦‚ç‡å‘ç”Ÿå¸‚åœºå†²å‡»
                    shock = np.random.choice([-0.3, -0.2, 0.2, 0.3])  # -30%åˆ°+30%çš„å†²å‡»
                    yearly_returns[i] += shock

            # ç°å®çš„æ”¶ç›Šç‡é™åˆ¶
            yearly_returns = np.clip(yearly_returns, -0.7, 1.5)  # é™åˆ¶åœ¨-70%åˆ°150%ä¹‹é—´

            # è®¡ç®—æŠ•èµ„ç»„åˆä»·å€¼è·¯å¾„
            portfolio_values = [self.initial_capital]
            for year_return in yearly_returns:
                new_value = portfolio_values[-1] * (1 + year_return)
                portfolio_values.append(new_value)

                # å¦‚æœä»·å€¼è·Œå¾—å¤ªä½ï¼Œè€ƒè™‘æ­¢æŸ
                if new_value < self.initial_capital * 0.2:  # è·Œç ´20%
                    break

            final_values.append(portfolio_values[-1])
            yearly_paths.append(portfolio_values[1:])  # å»æ‰åˆå§‹å€¼

            # è®¡ç®—æœ€å¤§å›æ’¤
            peak = np.maximum.accumulate(portfolio_values)
            drawdown = (np.array(portfolio_values) - peak) / peak
            max_drawdowns.append(np.min(drawdown))

        logger.info("ğŸ“ˆ è¿›è¡Œç»Ÿè®¡åˆ†æ...")

        # åŸºç¡€ç»Ÿè®¡
        final_values_array = np.array(final_values)
        percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]
        final_percentiles = np.percentile(final_values_array, percentiles)

        # æˆåŠŸæ¦‚ç‡åˆ†æ
        target_multipliers = [1.25, 1.5, 2.0, 3.0, 5.0, 10.0]
        multipliers = {}
        for multiplier in target_multipliers:
            target_value = self.initial_capital * multiplier
            success_count = sum(1 for value in final_values if value >= target_value)
            multipliers[f'{multiplier}x'] = success_count / simulations

        # å¤šå¹´åº¦åˆ†æ - ä¿®å¤æ•°ç»„ç»´åº¦ä¸ä¸€è‡´é—®é¢˜
        multi_year_analysis = {}
        if yearly_paths:
            # æ‰¾åˆ°æœ€çŸ­è·¯å¾„é•¿åº¦
            min_path_length = min(len(path) for path in yearly_paths)
            if min_path_length > 0:
                # åªå–å‰min_path_lengthå¹´çš„æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰è·¯å¾„é•¿åº¦ä¸€è‡´
                truncated_paths = [path[:min_path_length] for path in yearly_paths]
                yearly_array = np.array(truncated_paths)

                # åˆ†ææ‰€æœ‰å¯ç”¨å¹´ä»½ï¼Œæœ€å¤š5å¹´
                for year_idx in range(min(5, min_path_length)):
                    year_values = yearly_array[:, year_idx]
                    multi_year_analysis[f'year_{year_idx + 1}'] = {
                        'mean': np.mean(year_values),
                        'positive_return_prob': np.mean(year_values > self.initial_capital),
                        'doubling_prob': np.mean(year_values > self.initial_capital * 2)
                    }

                # ç¡®ä¿è‡³å°‘æœ‰3å¹´æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨é»˜è®¤å€¼å¡«å……
                for year_idx in range(len(multi_year_analysis), 3):
                    if year_idx == 0:
                        # ç¬¬1å¹´åŸºäºå†å²æ•°æ®ä¼°ç®—
                        estimated_return = self.initial_capital * (1 + annual_return)
                        estimated_vol = annual_volatility * self.initial_capital

                        # æ¨¡æ‹Ÿä¸€äº›æ•°æ®
                        simulated_values = np.random.normal(estimated_return, estimated_vol, 100)
                        simulated_values = np.maximum(simulated_values, self.initial_capital * 0.1)

                        multi_year_analysis[f'year_{year_idx + 1}'] = {
                            'mean': np.mean(simulated_values),
                            'positive_return_prob': np.mean(simulated_values > self.initial_capital),
                            'doubling_prob': np.mean(simulated_values > self.initial_capital * 2)
                        }
                    else:
                        # åç»­å¹´ä»½åŸºäºå‰ä¸€å¹´æ¨ç®—
                        prev_year = multi_year_analysis.get(f'year_{year_idx}', {})
                        prev_mean = prev_year.get('mean', self.initial_capital)

                        multi_year_analysis[f'year_{year_idx + 1}'] = {
                            'mean': prev_mean * (1 + annual_return * 0.8),  # ç•¥å¾®ä¿å®ˆçš„ä¼°ç®—
                            'positive_return_prob': max(0.3, prev_year.get('positive_return_prob', 0.7) - 0.1),
                            'doubling_prob': max(0.1, prev_year.get('doubling_prob', 0.3) - 0.1)
                        }

        # æƒ…æ™¯åˆ†æï¼ˆå·®å¼‚åŒ–ç‰ˆæœ¬ï¼‰
        scenario_analysis = {}

        # ç‰›å¸‚æƒ…æ™¯ï¼šæ”¶ç›Šæ›´é«˜ä½†æ³¢åŠ¨ä¹Ÿæ›´å¤§
        bull_return = annual_return * 1.3  # æ”¶ç›Šå¢åŠ 30%
        bull_vol = annual_volatility * 1.2  # æ³¢åŠ¨å¢åŠ 20%
        scenario_analysis['bull_market'] = {
            'success_probability': self._quick_scenario_calc(bull_return, bull_vol, years)
        }

        # ç†Šå¸‚æƒ…æ™¯ï¼šæ”¶ç›Šé™ä½ä¸”æ³¢åŠ¨å¢å¤§
        bear_return = max(0.05, annual_return * 0.6)  # æ”¶ç›Šé™ä½40%ï¼Œä½†æœ€ä½5%
        bear_vol = annual_volatility * 1.8  # æ³¢åŠ¨å¢åŠ 80%
        scenario_analysis['bear_market'] = {
            'success_probability': self._quick_scenario_calc(bear_return, bear_vol, years)
        }

        # é«˜æ³¢åŠ¨æƒ…æ™¯ï¼šæ³¢åŠ¨å¤§å¹…å¢åŠ ï¼Œæ”¶ç›Šç•¥é™
        high_vol_return = annual_return * 0.9  # æ”¶ç›Šé™ä½10%
        high_vol = annual_volatility * 2.5  # æ³¢åŠ¨å¢åŠ 150%
        scenario_analysis['high_volatility'] = {
            'success_probability': self._quick_scenario_calc(high_vol_return, high_vol, years)
        }

        # ä½æ³¢åŠ¨æƒ…æ™¯ï¼šæ³¢åŠ¨å¤§å¹…é™ä½ï¼Œæ”¶ç›Šä¹Ÿé™ä½
        low_vol_return = annual_return * 0.7  # æ”¶ç›Šé™ä½30%
        low_vol = annual_volatility * 0.4  # æ³¢åŠ¨é™ä½60%
        scenario_analysis['low_volatility'] = {
            'success_probability': self._quick_scenario_calc(low_vol_return, low_vol, years)
        }

        # é£é™©æŒ‡æ ‡
        risk_free_rate = 0.02
        total_returns = (final_values_array / self.initial_capital) ** (1/years) - 1
        excess_returns = total_returns - risk_free_rate
        sharpe_ratios = excess_returns / annual_volatility

        risk_metrics = {
            'max_drawdown_analysis': {
                'mean': np.mean(max_drawdowns),
                'worst_5_percent': np.percentile(max_drawdowns, 5)
            },
            'sharpe_ratio_distribution': {
                'mean': np.mean(sharpe_ratios),
                'positive_prob': np.mean(sharpe_ratios > 0)
            }
        }

        # ç®€åŒ–çš„åˆ†å¸ƒåˆ†æ
        distribution_analysis = {
            'tail_risk': {
                'var_95': np.percentile(final_values_array, 5),
                'cvar_95': np.mean(final_values_array[final_values_array <= np.percentile(final_values_array, 5)])
            }
        }

        logger.info("âœ… å¢é•¿é¢„æµ‹åˆ†æå®Œæˆ")

        return {
            'initial_capital': self.initial_capital,
            'annual_return': annual_return,
            'annual_volatility': annual_volatility,
            'years': years,
            'simulations': simulations,

            # åŸºç¡€ç»Ÿè®¡
            'final_value_statistics': {
                'mean': np.mean(final_values_array),
                'median': np.median(final_values_array),
                'std': np.std(final_values_array),
                'min': np.min(final_values_array),
                'max': np.max(final_values_array)
            },
            'final_value_percentiles': dict(zip(percentiles, final_percentiles)),

            # åˆ†æç»“æœ
            'multi_year_analysis': multi_year_analysis,
            'distribution_analysis': distribution_analysis,
            'risk_metrics': risk_metrics,
            'scenario_analysis': scenario_analysis,
            'success_analysis': {'target_multipliers': multipliers},

            # ä¿æŒå‘åå…¼å®¹
            'success_probability': multipliers.get('2.0x', 0),
            'success_probabilities': multipliers
        }

    def _quick_scenario_calc(self, annual_return: float, annual_volatility: float, years: int) -> float:
        """å¿«é€Ÿæƒ…æ™¯è®¡ç®— - æ›´ç°å®çš„ç‰ˆæœ¬ï¼Œè€ƒè™‘ä¸åŒæƒ…æ™¯çš„ç‰¹æ®Šçº¦æŸ"""
        test_simulations = 1000  # å¢åŠ æ¨¡æ‹Ÿæ¬¡æ•°
        success_count = 0

        for _ in range(test_simulations):
            # ç”Ÿæˆæµ‹è¯•è·¯å¾„ï¼Œæ·»åŠ æ›´å¤šç°å®çº¦æŸ
            test_returns = np.random.normal(annual_return, annual_volatility, years)

            # æ ¹æ®æ”¶ç›Šç‡æ°´å¹³è°ƒæ•´å†²å‡»æ¦‚ç‡å’Œå¼ºåº¦
            if annual_return > 0.5:  # è¶…é«˜æ”¶ç›Šç‡æƒ…æ™¯
                shock_prob = 0.25  # 25%æ¦‚ç‡å‘ç”Ÿå†²å‡»
                shock_choices = [-0.6, -0.4, -0.3, -0.2, 0.1, 0.2]  # æ›´åå‘è´Ÿé¢å†²å‡»
            elif annual_return > 0.3:  # é«˜æ”¶ç›Šç‡æƒ…æ™¯
                shock_prob = 0.2  # 20%æ¦‚ç‡å‘ç”Ÿå†²å‡»
                shock_choices = [-0.5, -0.3, -0.2, -0.1, 0.1, 0.3]
            elif annual_return < 0.2:  # ä½æ”¶ç›Šç‡æƒ…æ™¯
                shock_prob = 0.3  # 30%æ¦‚ç‡å‘ç”Ÿå†²å‡»
                shock_choices = [-0.4, -0.3, -0.2, 0.1, 0.2, 0.4]
            else:  # æ­£å¸¸æƒ…æ™¯
                shock_prob = 0.15
                shock_choices = [-0.4, -0.25, -0.15, 0.15, 0.25, 0.4]

            # æ·»åŠ éšæœºå¸‚åœºå†²å‡»
            for i in range(years):
                if np.random.random() < shock_prob:
                    shock = np.random.choice(shock_choices)
                    test_returns[i] += shock

            # æ›´ä¸¥æ ¼çš„æ”¶ç›Šç‡é™åˆ¶ï¼Œæ ¹æ®æƒ…æ™¯è°ƒæ•´
            if annual_return > 0.5:  # è¶…é«˜æ”¶ç›Šç‡æƒ…æ™¯ï¼Œæ›´ä¸¥æ ¼é™åˆ¶
                max_return = 0.8  # æœ€é«˜80%
            elif annual_return < 0.1:  # ä½æ”¶ç›Šç‡æƒ…æ™¯
                max_return = 0.5  # æœ€é«˜50%
            else:
                max_return = 1.2  # æ­£å¸¸120%

            test_returns = np.clip(test_returns, -0.9, max_return)

            # è®¡ç®—æœ€ç»ˆä»·å€¼
            final_value = self.initial_capital
            for year_return in test_returns:
                final_value *= (1 + year_return)

                # åŠ¨æ€æ­¢æŸæœºåˆ¶
                if final_value < self.initial_capital * 0.1:  # è·Œç ´10%æ­¢æŸ
                    final_value = self.initial_capital * 0.1
                    break

            if final_value >= self.initial_capital * 2:  # ç¿»å€
                success_count += 1

        return success_count / test_simulations

    def _generate_realistic_returns(self, annual_return: float, annual_volatility: float, total_steps: int) -> np.ndarray:
        """ç”Ÿæˆæ›´ç°å®çš„æ”¶ç›Šç‡è·¯å¾„ï¼ˆåŒ…å«å‡å€¼å›å½’å’Œæ³¢åŠ¨ç‡èšé›†ï¼‰"""
        dt = 1/252
        mean_reversion_speed = 0.1
        volatility_persistence = 0.9

        returns = np.zeros(total_steps)
        volatility_process = np.ones(total_steps) * annual_volatility

        for t in range(1, total_steps):
            drift = mean_reversion_speed * (annual_return/252 - returns[t-1]) * dt
            volatility_process[t] = (np.sqrt(volatility_persistence) * volatility_process[t-1] +
                                   np.sqrt(1 - volatility_persistence) * annual_volatility * np.random.randn())
            random_shock = volatility_process[t] / np.sqrt(252) * np.random.randn()
            returns[t] = returns[t-1] * (1 + drift * dt) + random_shock

        return returns

    def _calculate_rolling_volatility(self, returns: np.ndarray, window: int = 20) -> np.ndarray:
        """è®¡ç®—æ»šåŠ¨æ³¢åŠ¨ç‡"""
        if len(returns) < window:
            return np.array([np.std(returns)])

        rolling_vol = []
        for i in range(window, len(returns)):
            vol = np.std(returns[i-window:i]) * np.sqrt(252)
            rolling_vol.append(vol)

        return np.array(rolling_vol)

    def _analyze_multi_year_scenarios(self, yearly_values: np.ndarray, years: int) -> Dict[str, Any]:
        """åˆ†æå¤šæ—¶é—´ç»´åº¦æƒ…æ™¯"""
        analysis = {}

        for year in range(min(3, years)):
            if year < yearly_values.shape[1]:
                year_values = yearly_values[:, year]
                analysis[f'year_{year+1}'] = {
                    'mean': np.mean(year_values),
                    'median': np.median(year_values),
                    'std': np.std(year_values),
                    'percentiles': {
                        '10': np.percentile(year_values, 10),
                        '25': np.percentile(year_values, 25),
                        '75': np.percentile(year_values, 75),
                        '90': np.percentile(year_values, 90)
                    },
                    'positive_return_prob': np.mean(year_values > 1000000),
                    'doubling_prob': np.mean(year_values > 2000000)
                }

        return analysis

    def _analyze_return_distribution(self, final_values: list) -> Dict[str, Any]:
        """åˆ†ææ”¶ç›Šåˆ†å¸ƒç‰¹å¾"""
        final_values_array = np.array(final_values)
        log_returns = np.log(final_values_array / 1000000)

        try:
            from scipy import stats
            _, normality_p_value = stats.normaltest(log_returns)
            try:
                shape, loc, scale = stats.lognorm.fit(log_returns, floc=0)
                lognorm_params = {'shape': shape, 'loc': loc, 'scale': scale}
            except:
                lognorm_params = None
        except ImportError:
            normality_p_value = 1.0
            lognorm_params = None

        return {
            'log_returns_stats': {
                'mean': np.mean(log_returns),
                'std': np.std(log_returns),
                'skewness': self._calculate_skewness(log_returns),
                'kurtosis': self._calculate_kurtosis(log_returns)
            },
            'normality_test': {
                'is_normal': normality_p_value > 0.05,
                'p_value': normality_p_value
            },
            'distribution_fit': {
                'lognormal_params': lognorm_params,
                'best_fit': 'lognormal' if lognorm_params else 'unknown'
            },
            'tail_risk': {
                'var_95': np.percentile(final_values_array, 5),
                'var_99': np.percentile(final_values_array, 1),
                'cvar_95': np.mean(final_values_array[final_values_array <= np.percentile(final_values_array, 5)]),
                'cvar_99': np.mean(final_values_array[final_values_array <= np.percentile(final_values_array, 1)])
            }
        }

    def _calculate_risk_metrics(self, final_values: list, max_drawdowns: list,
                               annual_volatility: float) -> Dict[str, Any]:
        """è®¡ç®—è¯¦ç»†é£é™©æŒ‡æ ‡"""
        final_values_array = np.array(final_values)
        risk_free_rate = 0.02
        excess_returns = (final_values_array / 1000000) ** (1/10) - 1 - risk_free_rate
        sharpe_ratios = excess_returns / annual_volatility

        return {
            'max_drawdown_analysis': {
                'mean': np.mean(max_drawdowns),
                'median': np.median(max_drawdowns),
                'std': np.std(max_drawdowns),
                'worst_5_percent': np.percentile(max_drawdowns, 5),
                'worst_1_percent': np.percentile(max_drawdowns, 1)
            },
            'sharpe_ratio_distribution': {
                'mean': np.mean(sharpe_ratios),
                'median': np.median(sharpe_ratios),
                'std': np.std(sharpe_ratios),
                'positive_prob': np.mean(sharpe_ratios > 0)
            }
        }

    def _perform_scenario_analysis(self, annual_return: float, annual_volatility: float, years: int) -> Dict[str, Any]:
        """æƒ…æ™¯åˆ†æ"""
        scenarios = {}

        bull_return = annual_return * 1.5
        scenarios['bull_market'] = self._quick_scenario_calculation(bull_return, annual_volatility, years)

        bear_return = annual_return * 0.5
        bear_volatility = annual_volatility * 1.5
        scenarios['bear_market'] = self._quick_scenario_calculation(bear_return, bear_volatility, years)

        high_volatility = annual_volatility * 2.0
        scenarios['high_volatility'] = self._quick_scenario_calculation(annual_return, high_volatility, years)

        low_volatility = annual_volatility * 0.5
        scenarios['low_volatility'] = self._quick_scenario_calculation(annual_return, low_volatility, years)

        return scenarios

    def _quick_scenario_calculation(self, annual_return: float, annual_volatility: float, years: int,
                                   simulations: int = 1000) -> Dict[str, float]:
        """å¿«é€Ÿæƒ…æ™¯è®¡ç®—ï¼ˆç®€åŒ–ç‰ˆè’™ç‰¹å¡æ´›ï¼‰"""
        final_values = []

        for _ in range(simulations):
            yearly_returns = np.random.normal(annual_return, annual_volatility, years)
            final_value = 1000000 * np.prod(1 + yearly_returns)
            final_values.append(final_value)

        return {
            'mean_final_value': np.mean(final_values),
            'median_final_value': np.median(final_values),
            'success_probability': np.mean(np.array(final_values) > 2000000)
        }

    def _analyze_success_probabilities(self, final_values: list, years: int) -> Dict[str, Any]:
        """åˆ†æå„ç§æˆåŠŸæ¦‚ç‡"""
        success_analysis = {}

        target_multipliers = [1.25, 1.5, 2.0, 3.0, 5.0, 10.0]
        multipliers = {}
        for multiplier in target_multipliers:
            target_value = 1000000 * multiplier
            success_count = sum(1 for value in final_values if value >= target_value)
            multipliers[f'{multiplier}x'] = success_count / len(final_values)

        success_analysis['target_multipliers'] = multipliers

        return success_analysis

    def _analyze_time_series_features(self, all_paths: list) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´åºåˆ—ç‰¹å¾"""
        trends = []

        for path in all_paths:
            path_array = np.array(path)
            x = np.arange(len(path_array))
            slope, _ = np.polyfit(x, path_array, 1)
            trends.append(slope)

        return {
            'trend_analysis': {
                'mean_slope': np.mean(trends),
                'trend_consistency': np.mean(np.array(trends) > 0),
                'trend_volatility': np.std(trends)
            }
        }

    def _calculate_skewness(self, data: np.ndarray) -> float:
        """è®¡ç®—ååº¦"""
        mean = np.mean(data)
        std = np.std(data)
        return np.mean(((data - mean) / std) ** 3)

    def _calculate_kurtosis(self, data: np.ndarray) -> float:
        """è®¡ç®—å³°åº¦"""
        mean = np.mean(data)
        std = np.std(data)
        return np.mean(((data - mean) / std) ** 4) - 3

    def calculate_dollar_cost_averaging(self, monthly_investment: float,
                                      expected_return: float,
                                      expected_volatility: float,
                                      years: int = 10) -> Dict[str, Any]:
        """
        å®šæŠ•æ”¶ç›Šè®¡ç®—

        Args:
            monthly_investment: æ¯æœˆå®šæŠ•é‡‘é¢
            expected_return: é¢„æœŸå¹´åŒ–æ”¶ç›Šç‡
            expected_volatility: é¢„æœŸå¹´åŒ–æ³¢åŠ¨ç‡
            years: å®šæŠ•å¹´æ•°

        Returns:
            å®šæŠ•åˆ†æç»“æœ
        """
        months = years * 12
        monthly_return = expected_return / 12
        monthly_vol = expected_volatility / np.sqrt(12)

        # æ¨¡æ‹Ÿå®šæŠ•è·¯å¾„
        simulations = 1000
        final_values = []

        for _ in range(simulations):
            # ç”Ÿæˆæœˆåº¦æ”¶ç›Šç‡
            monthly_returns = np.random.normal(monthly_return, monthly_vol, months)

            # è®¡ç®—å®šæŠ•ç´¯è®¡ä»·å€¼
            total_invested = 0
            portfolio_value = 0

            for i, monthly_ret in enumerate(monthly_returns):
                total_invested += monthly_investment
                portfolio_value = (portfolio_value + monthly_investment) * (1 + monthly_ret)

            final_values.append(portfolio_value)

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        total_investment = monthly_investment * months
        avg_final_value = np.mean(final_values)
        avg_total_return = (avg_final_value - total_investment) / total_investment

        return {
            'monthly_investment': monthly_investment,
            'years': years,
            'total_months': months,
            'total_investment': total_investment,
            'expected_return': expected_return,
            'expected_volatility': expected_volatility,
            'average_final_value': avg_final_value,
            'average_total_return': avg_total_return,
            'simulations': simulations,
            'final_value_statistics': {
                'mean': np.mean(final_values),
                'median': np.median(final_values),
                'std': np.std(final_values),
                'min': np.min(final_values),
                'max': np.max(final_values)
            }
        }


class SignalGenerator:
    """æŠ•èµ„ä¿¡å·ç”Ÿæˆå™¨"""

    def __init__(self, ma_short: int = 20, ma_long: int = 60,
                 rsi_period: int = 14):
        """
        åˆå§‹åŒ–ä¿¡å·ç”Ÿæˆå™¨

        Args:
            ma_short: çŸ­æœŸç§»åŠ¨å¹³å‡çº¿å‘¨æœŸ
            ma_long: é•¿æœŸç§»åŠ¨å¹³å‡çº¿å‘¨æœŸ
            rsi_period: RSIå‘¨æœŸ
        """
        self.ma_short = ma_short
        self.ma_long = ma_long
        self.rsi_period = rsi_period

    def generate_ma_signals(self, prices: pd.Series) -> pd.DataFrame:
        """
        ç”Ÿæˆç§»åŠ¨å¹³å‡çº¿ä¿¡å·

        Args:
            prices: ä»·æ ¼åºåˆ—

        Returns:
            ä¿¡å·DataFrame
        """
        signals = pd.DataFrame(index=prices.index)
        signals['price'] = prices

        # è®¡ç®—ç§»åŠ¨å¹³å‡çº¿
        signals['ma_short'] = prices.rolling(window=self.ma_short).mean()
        signals['ma_long'] = prices.rolling(window=self.ma_long).mean()

        # ç”Ÿæˆä¿¡å·
        signals['signal'] = 0
        signals.loc[signals['ma_short'] > signals['ma_long'], 'signal'] = 1
        signals.loc[signals['ma_short'] < signals['ma_long'], 'signal'] = -1

        # è®¡ç®—ä¿¡å·å˜åŒ–
        signals['signal_change'] = signals['signal'].diff()

        return signals

    def generate_rsi_signals(self, prices: pd.Series,
                           oversold: float = 30,
                           overbought: float = 70) -> pd.DataFrame:
        """
        ç”ŸæˆRSIä¿¡å·

        Args:
            prices: ä»·æ ¼åºåˆ—
            oversold: è¶…å–çº¿
            overbought: è¶…ä¹°çº¿

        Returns:
            ä¿¡å·DataFrame
        """
        signals = pd.DataFrame(index=prices.index)
        signals['price'] = prices

        # è®¡ç®—RSI
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=self.rsi_period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=self.rsi_period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))

        signals['rsi'] = rsi

        # ç”Ÿæˆä¿¡å·
        signals['signal'] = 0
        signals.loc[rsi < oversold, 'signal'] = 1  # è¶…å–ä¹°å…¥
        signals.loc[rsi > overbought, 'signal'] = -1  # è¶…ä¹°å–å‡º

        return signals

    def generate_portfolio_rebalance_signal(self, current_weights: np.ndarray,
                                         target_weights: np.ndarray,
                                         threshold: float = 0.05) -> Dict[str, Any]:
        """
        ç”ŸæˆæŠ•èµ„ç»„åˆå†å¹³è¡¡ä¿¡å·

        Args:
            current_weights: å½“å‰æƒé‡
            target_weights: ç›®æ ‡æƒé‡
            threshold: æƒé‡åç¦»é˜ˆå€¼

        Returns:
            å†å¹³è¡¡ä¿¡å·
        """
        weight_diff = target_weights - current_weights
        abs_diff = np.abs(weight_diff)

        max_diff = np.max(abs_diff)
        needs_rebalancing = max_diff > threshold

        signal_strength = max_diff / threshold if threshold > 0 else 0

        return {
            'needs_rebalancing': needs_rebalancing,
            'signal_strength': signal_strength,
            'max_deviation': max_diff,
            'weight_differences': weight_diff,
            'absolute_differences': abs_diff,
            'deviating_assets': np.where(abs_diff > threshold)[0].tolist()
        }


class PerformanceAttribution:
    """ä¸šç»©å½’å› åˆ†æ"""

    def __init__(self, benchmark_weights: Optional[np.ndarray] = None):
        """
        åˆå§‹åŒ–ä¸šç»©å½’å› åˆ†æå™¨

        Args:
            benchmark_weights: åŸºå‡†æƒé‡
        """
        self.benchmark_weights = benchmark_weights

    def calculate_attribution(self, portfolio_returns: pd.Series,
                            benchmark_returns: pd.Series,
                            asset_returns: pd.DataFrame,
                            portfolio_weights: np.ndarray,
                            benchmark_weights: np.ndarray) -> Dict[str, Any]:
        """
        è®¡ç®—ä¸šç»©å½’å› 

        Args:
            portfolio_returns: æŠ•èµ„ç»„åˆæ”¶ç›Šç‡
            benchmark_returns: åŸºå‡†æ”¶ç›Šç‡
            asset_returns: å„èµ„äº§æ”¶ç›Šç‡
            portfolio_weights: æŠ•èµ„ç»„åˆæƒé‡
            benchmark_weights: åŸºå‡†æƒé‡

        Returns:
            ä¸šç»©å½’å› ç»“æœ
        """
        # è®¡ç®—è¶…é¢æ”¶ç›Š
        excess_return = portfolio_returns - benchmark_returns

        # 1. èµ„äº§é…ç½®æ•ˆåº”
        allocation_effect = self._calculate_allocation_effect(
            asset_returns, portfolio_weights, benchmark_weights
        )

        # 2. é€‰è‚¡æ•ˆåº”
        selection_effect = self._calculate_selection_effect(
            asset_returns, portfolio_weights, benchmark_weights
        )

        # 3. äº¤äº’æ•ˆåº”
        interaction_effect = self._calculate_interaction_effect(
            asset_returns, portfolio_weights, benchmark_weights
        )

        # 4. æ€»æ•ˆåº”éªŒè¯
        total_effect = allocation_effect + selection_effect + interaction_effect

        return {
            'total_excess_return': excess_return.mean(),
            'allocation_effect': allocation_effect,
            'selection_effect': selection_effect,
            'interaction_effect': interaction_effect,
            'total_effect': total_effect,
            'attribution_breakdown': {
                'allocation_pct': allocation_effect / total_effect * 100 if total_effect != 0 else 0,
                'selection_pct': selection_effect / total_effect * 100 if total_effect != 0 else 0,
                'interaction_pct': interaction_effect / total_effect * 100 if total_effect != 0 else 0
            }
        }

    def _calculate_allocation_effect(self, asset_returns: pd.DataFrame,
                                   portfolio_weights: np.ndarray,
                                   benchmark_weights: np.ndarray) -> float:
        """è®¡ç®—èµ„äº§é…ç½®æ•ˆåº”"""
        benchmark_returns = asset_returns.mean()
        weight_diff = portfolio_weights - benchmark_weights
        return np.sum(weight_diff * benchmark_returns)

    def _calculate_selection_effect(self, asset_returns: pd.DataFrame,
                                  portfolio_weights: np.ndarray,
                                  benchmark_weights: np.ndarray) -> float:
        """è®¡ç®—é€‰è‚¡æ•ˆåº”"""
        asset_excess_returns = asset_returns.mean() - asset_returns.mean().mean()
        return np.sum(benchmark_weights * asset_excess_returns)

    def _calculate_interaction_effect(self, asset_returns: pd.DataFrame,
                                    portfolio_weights: np.ndarray,
                                    benchmark_weights: np.ndarray) -> float:
        """è®¡ç®—äº¤äº’æ•ˆåº”"""
        asset_excess_returns = asset_returns.mean() - asset_returns.mean().mean()
        weight_diff = portfolio_weights - benchmark_weights
        return np.sum(weight_diff * asset_excess_returns)

    def calculate_contribution_analysis(self, portfolio_returns: pd.Series,
                                      weights: np.ndarray,
                                      asset_returns: pd.DataFrame) -> Dict[str, float]:
        """
        è®¡ç®—å„èµ„äº§è´¡çŒ®åº¦åˆ†æ

        Args:
            portfolio_returns: æŠ•èµ„ç»„åˆæ”¶ç›Šç‡
            weights: æƒé‡
            asset_returns: å„èµ„äº§æ”¶ç›Šç‡

        Returns:
            è´¡çŒ®åº¦åˆ†æç»“æœ
        """
        contributions = {}
        total_return = portfolio_returns.mean()

        for i, (weight, asset_name) in enumerate(zip(weights, asset_returns.columns)):
            asset_contribution = weight * asset_returns[asset_name].mean()
            contribution_pct = asset_contribution / total_return * 100 if total_return != 0 else 0

            contributions[asset_name] = {
                'weight': weight,
                'asset_return': asset_returns[asset_name].mean(),
                'contribution': asset_contribution,
                'contribution_pct': contribution_pct
            }

        return contributions


class PortfolioAnalyzer:
    """æŠ•èµ„ç»„åˆåˆ†æå™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æŠ•èµ„ç»„åˆåˆ†æå™¨"""
        pass

    def analyze_sector_exposure(self, etf_codes: List[str],
                              weights: np.ndarray) -> Dict[str, Any]:
        """
        åˆ†æè¡Œä¸šæ•å£ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰

        Args:
            etf_codes: ETFä»£ç åˆ—è¡¨
            weights: æƒé‡

        Returns:
            è¡Œä¸šæ•å£åˆ†æ
        """
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦ETFçš„è¡Œä¸šåˆ†ç±»æ•°æ®
        sector_mapping = self._get_etf_sector_mapping()

        sector_exposure = {}
        for etf_code, weight in zip(etf_codes, weights):
            sector = sector_mapping.get(etf_code, 'å…¶ä»–')
            if sector not in sector_exposure:
                sector_exposure[sector] = 0
            sector_exposure[sector] += weight

        # è®¡ç®—é›†ä¸­åº¦æŒ‡æ ‡
        herfindahl_index = sum(exposure ** 2 for exposure in sector_exposure.values())

        return {
            'sector_exposure': sector_exposure,
            'herfindahl_index': herfindahl_index,
            'max_sector_exposure': max(sector_exposure.values()),
            'sector_count': len(sector_exposure)
        }

    def _get_etf_sector_mapping(self) -> Dict[str, str]:
        """è·å–ETFè¡Œä¸šæ˜ å°„ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦å®Œæ•´çš„ETFè¡Œä¸šåˆ†ç±»æ•°æ®
        return {
            '159632.SZ': 'æ–°èƒ½æº',
            '159670.SZ': 'åŠå¯¼ä½“',
            '159770.SZ': 'æ¶ˆè´¹',
            '159995.SZ': 'èŠ¯ç‰‡',
            '159871.SZ': 'æ–°èƒ½æºè½¦',
            '510210.SH': 'å›½å€º'
        }

    def generate_investment_recommendations(self, risk_metrics: Dict[str, Any],
                                         performance_metrics: Dict[str, Any],
                                         market_conditions: str = 'normal') -> List[str]:
        """
        ç”ŸæˆæŠ•èµ„å»ºè®®

        Args:
            risk_metrics: é£é™©æŒ‡æ ‡
            performance_metrics: ç»©æ•ˆæŒ‡æ ‡
            market_conditions: å¸‚åœºæ¡ä»¶

        Returns:
            æŠ•èµ„å»ºè®®åˆ—è¡¨
        """
        recommendations = []

        # åŸºäºé£é™©æŒ‡æ ‡çš„å»ºè®®
        overall_risk = risk_metrics.get('risk_rating', {}).get('overall_risk', 'ä¸­é£é™©')
        if overall_risk == 'é«˜é£é™©':
            recommendations.append("ç»„åˆé£é™©è¾ƒé«˜ï¼Œå»ºè®®è€ƒè™‘é™ä½ä»“ä½æˆ–å¢åŠ é˜²å¾¡æ€§èµ„äº§")
        elif overall_risk == 'ä½é£é™©':
            recommendations.append("ç»„åˆé£é™©è¾ƒä½ï¼Œå¯é€‚å½“å¢åŠ æˆé•¿æ€§èµ„äº§é…ç½®")

        # åŸºäºé›†ä¸­åº¦é£é™©çš„å»ºè®®
        concentration_risk = risk_metrics.get('concentration_risk', {})
        if concentration_risk.get('hhi', 0) > 3000:
            recommendations.append("æŒä»“é›†ä¸­åº¦è¿‡é«˜ï¼Œå»ºè®®åˆ†æ•£åŒ–æŠ•èµ„")

        # åŸºäºç»©æ•ˆæŒ‡æ ‡çš„å»ºè®®
        sharpe_ratio = performance_metrics.get('sharpe_ratio', 0)
        if sharpe_ratio < 0.5:
            recommendations.append("å¤æ™®æ¯”ç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–èµ„äº§é…ç½®ä»¥æé«˜é£é™©è°ƒæ•´åæ”¶ç›Š")

        max_drawdown = performance_metrics.get('max_drawdown', 0)
        if max_drawdown < -0.20:
            recommendations.append("æœ€å¤§å›æ’¤è¾ƒå¤§ï¼Œå»ºè®®åŠ å¼ºé£é™©æ§åˆ¶")

        # åŸºäºå¸‚åœºæ¡ä»¶çš„å»ºè®®
        if market_conditions == 'bull':
            recommendations.append("å¸‚åœºå¤„äºä¸Šæ¶¨è¶‹åŠ¿ï¼Œå¯é€‚å½“å¢åŠ æƒç›Šç±»èµ„äº§é…ç½®")
        elif market_conditions == 'bear':
            recommendations.append("å¸‚åœºå¤„äºä¸‹è·Œè¶‹åŠ¿ï¼Œå»ºè®®å¢åŠ é˜²å¾¡æ€§èµ„äº§æˆ–ç°é‡‘æŒæœ‰")
        elif market_conditions == 'volatile':
            recommendations.append("å¸‚åœºæ³¢åŠ¨è¾ƒå¤§ï¼Œå»ºè®®ä¿æŒè°¨æ…ï¼Œé€‚å½“é™ä½ä»“ä½")

        return recommendations


def get_investment_calculator(initial_capital: float = 1000000) -> InvestmentCalculator:
    """è·å–æŠ•èµ„è®¡ç®—å™¨å®ä¾‹"""
    return InvestmentCalculator(initial_capital)


def get_signal_generator(ma_short: int = 20, ma_long: int = 60,
                        rsi_period: int = 14) -> SignalGenerator:
    """è·å–ä¿¡å·ç”Ÿæˆå™¨å®ä¾‹"""
    return SignalGenerator(ma_short, ma_long, rsi_period)


def get_performance_attribution(benchmark_weights: Optional[np.ndarray] = None) -> PerformanceAttribution:
    """è·å–ä¸šç»©å½’å› åˆ†æå™¨å®ä¾‹"""
    return PerformanceAttribution(benchmark_weights)


def get_portfolio_analyzer() -> PortfolioAnalyzer:
    """è·å–æŠ•èµ„ç»„åˆåˆ†æå™¨å®ä¾‹"""
    return PortfolioAnalyzer()


# === å¢å¼ºå¢é•¿é¢„æµ‹çš„è¾…åŠ©æ–¹æ³• ===

def _generate_realistic_returns(self, annual_return: float, annual_volatility: float, total_steps: int) -> np.ndarray:
    """
    ç”Ÿæˆæ›´ç°å®çš„æ”¶ç›Šç‡è·¯å¾„ï¼ˆåŒ…å«å‡å€¼å›å½’å’Œæ³¢åŠ¨ç‡èšé›†ï¼‰

    Args:
        annual_return: å¹´åŒ–æ”¶ç›Šç‡
        annual_volatility: å¹´åŒ–æ³¢åŠ¨ç‡
        total_steps: æ€»æ­¥æ•°

    Returns:
        æ—¥æ”¶ç›Šç‡åºåˆ—
    """
    dt = 1/252

    # Ornstein-Uhlenbeckè¿‡ç¨‹çš„å‡å€¼å›å½’å‚æ•°
    mean_reversion_speed = 0.1  # å‡å€¼å›å½’é€Ÿåº¦
    volatility_persistence = 0.9  # æ³¢åŠ¨ç‡èšé›†æŒä¹…æ€§

    # åˆå§‹åŒ–
    returns = np.zeros(total_steps)
    volatility_process = np.ones(total_steps) * annual_volatility

    # ç”Ÿæˆéšæœºè·¯å¾„
    for t in range(1, total_steps):
        # å‡å€¼å›å½’çš„çŸ­æœŸæ”¶ç›Šç‡
        drift = mean_reversion_speed * (annual_return/252 - returns[t-1]) * dt

        # GARCH-likeæ³¢åŠ¨ç‡è¿‡ç¨‹
        volatility_process[t] = (np.sqrt(volatility_persistence) * volatility_process[t-1] +
                               np.sqrt(1 - volatility_persistence) * annual_volatility * np.random.randn())

        # ç”Ÿæˆæ”¶ç›Šç‡
        random_shock = volatility_process[t] / np.sqrt(252) * np.random.randn()
        returns[t] = returns[t-1] * (1 + drift * dt) + random_shock

    return returns


def _calculate_rolling_volatility(self, returns: np.ndarray, window: int = 20) -> np.ndarray:
    """è®¡ç®—æ»šåŠ¨æ³¢åŠ¨ç‡"""
    if len(returns) < window:
        return np.array([np.std(returns)])

    rolling_vol = []
    for i in range(window, len(returns)):
        vol = np.std(returns[i-window:i]) * np.sqrt(252)  # å¹´åŒ–
        rolling_vol.append(vol)

    return np.array(rolling_vol)


def _analyze_multi_year_scenarios(self, yearly_values: np.ndarray, years: int) -> Dict[str, Any]:
    """åˆ†æå¤šæ—¶é—´ç»´åº¦æƒ…æ™¯"""
    analysis = {}

    for year in range(min(3, years)):  # åˆ†æå‰3å¹´
        if year < yearly_values.shape[1]:
            year_values = yearly_values[:, year]
            analysis[f'year_{year+1}'] = {
                'mean': np.mean(year_values),
                'median': np.median(year_values),
                'std': np.std(year_values),
                'percentiles': {
                    '10': np.percentile(year_values, 10),
                    '25': np.percentile(year_values, 25),
                    '75': np.percentile(year_values, 75),
                    '90': np.percentile(year_values, 90)
                },
                'positive_return_prob': np.mean(year_values > 1000000),  # è¶…è¿‡åˆå§‹æŠ•èµ„æ¦‚ç‡
                'doubling_prob': np.mean(year_values > 2000000)  # ç¿»å€æ¦‚ç‡
            }

    # å¹´åº¦å¢é•¿è·¯å¾„åˆ†æ
    annual_returns = []
    for path in yearly_values:
        path_returns = []
        for i in range(1, len(path)):
            if path[i-1] > 0:
                path_returns.append((path[i] - path[i-1]) / path[i-1])
        if path_returns:
            annual_returns.extend(path_returns)

    if annual_returns:
        analysis['annual_return_distribution'] = {
            'mean': np.mean(annual_returns),
            'std': np.std(annual_returns),
            'negative_years_prob': np.mean(np.array(annual_returns) < 0)
        }

    return analysis


def _analyze_return_distribution(self, final_values: np.ndarray) -> Dict[str, Any]:
    """åˆ†ææ”¶ç›Šåˆ†å¸ƒç‰¹å¾"""
    # è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡
    log_returns = np.log(final_values / 1000000)  # ç›¸å¯¹äºåˆå§‹æŠ•èµ„

    # æ­£æ€æ€§æ£€éªŒ
    from scipy import stats
    _, normality_p_value = stats.normaltest(log_returns)

    # åˆ†å¸ƒæ‹Ÿåˆ
    try:
        # å°è¯•æ‹Ÿåˆå¯¹æ•°æ­£æ€åˆ†å¸ƒ
        shape, loc, scale = stats.lognorm.fit(log_returns, floc=0)
        lognorm_params = {'shape': shape, 'loc': loc, 'scale': scale}
    except:
        lognorm_params = None

    return {
        'log_returns_stats': {
            'mean': np.mean(log_returns),
            'std': np.std(log_returns),
            'skewness': stats.skew(log_returns),
            'kurtosis': stats.kurtosis(log_returns)
        },
        'normality_test': {
            'is_normal': normality_p_value > 0.05,
            'p_value': normality_p_value
        },
        'distribution_fit': {
            'lognormal_params': lognorm_params,
            'best_fit': 'lognormal' if lognorm_params else 'unknown'
        },
        'tail_risk': {
            'var_95': np.percentile(final_values, 5),
            'var_99': np.percentile(final_values, 1),
            'cvar_95': np.mean(final_values[final_values <= np.percentile(final_values, 5)]),
            'cvar_99': np.mean(final_values[final_values <= np.percentile(final_values, 1)])
        }
    }


def _calculate_risk_metrics(self, final_values: np.ndarray, max_drawdowns: list,
                           annual_volatility: float) -> Dict[str, Any]:
    """è®¡ç®—è¯¦ç»†é£é™©æŒ‡æ ‡"""
    # è®¡ç®—å¤æ™®æ¯”ç‡åˆ†å¸ƒ
    risk_free_rate = 0.02
    excess_returns = (final_values / 1000000) ** (1/10) - 1 - risk_free_rate
    sharpe_ratios = excess_returns / annual_volatility

    return {
        'max_drawdown_analysis': {
            'mean': np.mean(max_drawdowns),
            'median': np.median(max_drawdowns),
            'std': np.std(max_drawdowns),
            'worst_5_percent': np.percentile(max_drawdowns, 5),
            'worst_1_percent': np.percentile(max_drawdowns, 1)
        },
        'sharpe_ratio_distribution': {
            'mean': np.mean(sharpe_ratios),
            'median': np.median(sharpe_ratios),
            'std': np.std(sharpe_ratios),
            'positive_prob': np.mean(sharpe_ratios > 0)
        },
        'risk_adjusted_metrics': {
            'sortino_ratio_mean': np.mean([r / annual_volatility for r in excess_returns if r > 0]) if any(excess_returns > 0) else 0,
            'calmar_ratio_mean': np.mean(excess_returns) / abs(np.mean(max_drawdowns)) if np.mean(max_drawdowns) != 0 else 0
        }
    }


def _perform_scenario_analysis(self, annual_return: float, annual_volatility: float, years: int) -> Dict[str, Any]:
    """æƒ…æ™¯åˆ†æ"""
    scenarios = {}

    # ç‰›å¸‚æƒ…æ™¯ï¼šæ”¶ç›Šç‡+50%ï¼Œæ³¢åŠ¨ç‡ä¸å˜
    bull_return = annual_return * 1.5
    scenarios['bull_market'] = self._quick_scenario_calculation(bull_return, annual_volatility, years)

    # ç†Šå¸‚æƒ…æ™¯ï¼šæ”¶ç›Šç‡-50%ï¼Œæ³¢åŠ¨ç‡+50%
    bear_return = annual_return * 0.5
    bear_volatility = annual_volatility * 1.5
    scenarios['bear_market'] = self._quick_scenario_calculation(bear_return, bear_volatility, years)

    # é«˜æ³¢åŠ¨æƒ…æ™¯ï¼šæ”¶ç›Šç‡ä¸å˜ï¼Œæ³¢åŠ¨ç‡+100%
    high_vol_return = annual_return
    high_volatility = annual_volatility * 2.0
    scenarios['high_volatility'] = self._quick_scenario_calculation(high_vol_return, high_volatility, years)

    # ä½æ³¢åŠ¨æƒ…æ™¯ï¼šæ”¶ç›Šç‡ä¸å˜ï¼Œæ³¢åŠ¨ç‡-50%
    low_vol_return = annual_return
    low_volatility = annual_volatility * 0.5
    scenarios['low_volatility'] = self._quick_scenario_calculation(low_vol_return, low_volatility, years)

    return scenarios


def _quick_scenario_calculation(self, annual_return: float, annual_volatility: float, years: int,
                               simulations: int = 1000) -> Dict[str, float]:
    """å¿«é€Ÿæƒ…æ™¯è®¡ç®—ï¼ˆç®€åŒ–ç‰ˆè’™ç‰¹å¡æ´›ï¼‰"""
    final_values = []

    for _ in range(simulations):
        # ç®€åŒ–çš„å¹´å¤åˆ©è®¡ç®—
        yearly_returns = np.random.normal(annual_return, annual_volatility, years)
        final_value = 1000000 * np.prod(1 + yearly_returns)
        final_values.append(final_value)

    return {
        'mean_final_value': np.mean(final_values),
        'median_final_value': np.median(final_values),
        'success_probability': np.mean(np.array(final_values) > 2000000)  # ç¿»å€æ¦‚ç‡
    }


def _analyze_success_probabilities(self, final_values: np.ndarray, years: int) -> Dict[str, Any]:
    """åˆ†æå„ç§æˆåŠŸæ¦‚ç‡"""
    success_analysis = {}

    # å¤šå€æ•°æˆåŠŸæ¦‚ç‡
    target_multipliers = [1.25, 1.5, 2.0, 3.0, 5.0, 10.0]
    multipliers = {}
    for multiplier in target_multipliers:
        target_value = 1000000 * multiplier
        success_count = sum(1 for value in final_values if value >= target_value)
        multipliers[f'{multiplier}x'] = success_count / len(final_values)

    success_analysis['target_multipliers'] = multipliers

    # ä¸åŒæ—¶é—´ç‚¹çš„æˆåŠŸæ¦‚ç‡
    break_even = {}
    for years_target in [3, 5, 7, 10]:
        if years_target <= years:
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…è·¯å¾„è®¡ç®—ï¼Œç®€åŒ–å¤„ç†
            break_even[f'{years_target}y'] = {
                '1.5x': multipliers.get('1.5x', 0) * (years_target / years),  # ç®€åŒ–ä¼°ç®—
                '2.0x': multipliers.get('2.0x', 0) * (years_target / years)
            }

    success_analysis['break_even'] = break_even

    # é£é™©è°ƒæ•´æˆåŠŸç‡
    # è€ƒè™‘æœ€å¤§å›æ’¤é™åˆ¶çš„æˆåŠŸç‡
    risk_adjusted_success = {}
    for multiplier in [1.5, 2.0, 3.0]:
        # ç®€åŒ–ï¼šå‡è®¾åœ¨å›æ’¤é™åˆ¶ä¸‹çš„æˆåŠŸç‡ä¼šé™ä½
        base_success = multipliers.get(f'{multiplier}x', 0)
        risk_adjusted_success[f'{multiplier}x_10dd_limit'] = base_success * 0.8  # å‡è®¾20%çš„æ¦‚ç‡ä¼šè¿åå›æ’¤é™åˆ¶

    success_analysis['risk_adjusted'] = risk_adjusted_success

    return success_analysis


def _analyze_time_series_features(self, all_paths: np.ndarray) -> Dict[str, Any]:
    """åˆ†ææ—¶é—´åºåˆ—ç‰¹å¾"""
    # è®¡ç®—è·¯å¾„çš„æŒç»­æ€§ã€è¶‹åŠ¿å¼ºåº¦ç­‰
    trends = []
    volatilities = []

    for path in all_paths:
        # è®¡ç®—è¶‹åŠ¿å¼ºåº¦ï¼ˆä½¿ç”¨çº¿æ€§å›å½’æ–œç‡ï¼‰
        x = np.arange(len(path))
        slope, _ = np.polyfit(x, path, 1)
        trends.append(slope)

        # è®¡ç®—è·¯å¾„æ³¢åŠ¨ç‡
        returns = np.diff(path) / path[:-1]
        vol = np.std(returns) * np.sqrt(252)
        volatilities.append(vol)

    return {
        'trend_analysis': {
            'mean_slope': np.mean(trends),
            'trend_consistency': np.mean(np.array(trends) > 0),  # ä¸Šå‡è¶‹åŠ¿çš„æ¯”ä¾‹
            'trend_volatility': np.std(trends)
        },
        'path_volatility_analysis': {
            'mean_path_volatility': np.mean(volatilities),
            'volatility_of_volatility': np.std(volatilities),
            'volatility_persistence': self._calculate_autocorrelation(volatilities, 1)
        }
    }


def _calculate_skewness(self, data: np.ndarray) -> float:
    """è®¡ç®—ååº¦"""
    mean = np.mean(data)
    std = np.std(data)
    return np.mean(((data - mean) / std) ** 3)


def _calculate_kurtosis(self, data: np.ndarray) -> float:
    """è®¡ç®—å³°åº¦"""
    mean = np.mean(data)
    std = np.std(data)
    return np.mean(((data - mean) / std) ** 4) - 3  # å‡å»3å¾—åˆ°è¶…é¢å³°åº¦


def _calculate_autocorrelation(self, series: list, lag: int) -> float:
    """è®¡ç®—è‡ªç›¸å…³ç³»æ•°"""
    if len(series) <= lag:
        return 0

    series_array = np.array(series)
    series_lag = series_array[:-lag] if lag > 0 else series_array
    series_lead = series_array[lag:] if lag > 0 else series_array

    if len(series_lag) == 0 or len(series_lead) == 0:
        return 0

    correlation = np.corrcoef(series_lag, series_lead)[0, 1]
    return correlation if not np.isnan(correlation) else 0